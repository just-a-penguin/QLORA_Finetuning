{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install  peft -q\n",
    "!pip install -U bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T15:06:33.065334Z",
     "iopub.status.busy": "2024-11-04T15:06:33.064624Z",
     "iopub.status.idle": "2024-11-04T15:06:51.862407Z",
     "shell.execute_reply": "2024-11-04T15:06:51.861535Z",
     "shell.execute_reply.started": "2024-11-04T15:06:33.065266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-04T18:22:45.236031Z",
     "iopub.status.busy": "2024-11-04T18:22:45.235363Z",
     "iopub.status.idle": "2024-11-04T18:22:46.417514Z",
     "shell.execute_reply": "2024-11-04T18:22:46.416645Z",
     "shell.execute_reply.started": "2024-11-04T18:22:45.235981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550152\n",
      "Training Data: 1001 samples\n",
      "Testing Data: 100 samples\n",
      "Validation Data: 100 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "ds = load_dataset(\"stanfordnlp/snli\")\n",
    "\n",
    "# Function to sample data based on step size and sample size\n",
    "def sample_data(dataset, sample_size, step):\n",
    "    sampled_indices = range(0, len(dataset), step)[:sample_size]\n",
    "    return dataset.select(sampled_indices)\n",
    "\n",
    "# Create sampled subsets for training, testing, and validation\n",
    "train_data = sample_data(ds['train'], sample_size=550_000, step=550)\n",
    "test_data = sample_data(ds['test'], sample_size=10_000, step=100)\n",
    "validation_data = sample_data(ds['validation'], sample_size=1_000, step=100)\n",
    "\n",
    "# Display a summary of the datasets\n",
    "print(len(ds[\"train\"]))\n",
    "print(f\"Training Data: {len(train_data)} samples\")\n",
    "print(f\"Testing Data: {len(test_data)} samples\")\n",
    "print(f\"Validation Data: {len(validation_data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds[\"test\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T16:08:21.431502Z",
     "iopub.status.busy": "2024-11-04T16:08:21.431148Z",
     "iopub.status.idle": "2024-11-04T16:08:27.352797Z",
     "shell.execute_reply": "2024-11-04T16:08:27.351916Z",
     "shell.execute_reply.started": "2024-11-04T16:08:21.431466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe9e7588bd34cd78a3344829c7da788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-2\"\n",
    "\n",
    "# Define 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the pretrained model with 4-bit quantization\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# LoRA configuration for QLoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Load and configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T16:09:29.354266Z",
     "iopub.status.busy": "2024-11-04T16:09:29.353866Z",
     "iopub.status.idle": "2024-11-04T16:10:29.630390Z",
     "shell.execute_reply": "2024-11-04T16:10:29.629229Z",
     "shell.execute_reply.started": "2024-11-04T16:09:29.354221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test Samples:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   1%|          | 1/100 [00:00<00:58,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   2%|▏         | 2/100 [00:01<00:57,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   3%|▎         | 3/100 [00:01<00:57,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   4%|▍         | 4/100 [00:02<00:59,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   5%|▌         | 5/100 [00:03<00:58,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   6%|▌         | 6/100 [00:03<00:57,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   7%|▋         | 7/100 [00:04<00:56,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   8%|▊         | 8/100 [00:04<00:55,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   9%|▉         | 9/100 [00:05<00:54,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  10%|█         | 10/100 [00:06<00:53,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  11%|█         | 11/100 [00:06<00:52,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  12%|█▏        | 12/100 [00:07<00:52,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  13%|█▎        | 13/100 [00:07<00:53,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  14%|█▍        | 14/100 [00:08<00:52,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  15%|█▌        | 15/100 [00:09<00:51,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  16%|█▌        | 16/100 [00:09<00:50,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  17%|█▋        | 17/100 [00:10<00:49,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  18%|█▊        | 18/100 [00:10<00:48,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  19%|█▉        | 19/100 [00:11<00:48,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  20%|██        | 20/100 [00:11<00:47,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  21%|██        | 21/100 [00:12<00:46,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  22%|██▏       | 22/100 [00:13<00:47,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  23%|██▎       | 23/100 [00:13<00:46,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  24%|██▍       | 24/100 [00:14<00:45,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  25%|██▌       | 25/100 [00:15<00:45,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  26%|██▌       | 26/100 [00:15<00:45,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  27%|██▋       | 27/100 [00:16<00:44,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  28%|██▊       | 28/100 [00:16<00:43,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  29%|██▉       | 29/100 [00:17<00:42,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  30%|███       | 30/100 [00:18<00:42,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  31%|███       | 31/100 [00:18<00:41,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  32%|███▏      | 32/100 [00:19<00:41,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  33%|███▎      | 33/100 [00:19<00:40,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  34%|███▍      | 34/100 [00:20<00:39,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  35%|███▌      | 35/100 [00:21<00:38,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  36%|███▌      | 36/100 [00:21<00:38,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  37%|███▋      | 37/100 [00:22<00:37,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  38%|███▊      | 38/100 [00:22<00:36,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  39%|███▉      | 39/100 [00:23<00:38,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  40%|████      | 40/100 [00:24<00:38,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  41%|████      | 41/100 [00:24<00:36,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  42%|████▏     | 42/100 [00:25<00:35,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  43%|████▎     | 43/100 [00:26<00:34,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  44%|████▍     | 44/100 [00:26<00:33,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  45%|████▌     | 45/100 [00:27<00:33,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  46%|████▌     | 46/100 [00:27<00:32,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  47%|████▋     | 47/100 [00:28<00:31,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  48%|████▊     | 48/100 [00:28<00:31,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  49%|████▉     | 49/100 [00:29<00:30,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  50%|█████     | 50/100 [00:30<00:29,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  51%|█████     | 51/100 [00:30<00:29,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  52%|█████▏    | 52/100 [00:31<00:28,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  53%|█████▎    | 53/100 [00:31<00:27,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  54%|█████▍    | 54/100 [00:32<00:27,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  55%|█████▌    | 55/100 [00:33<00:26,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  56%|█████▌    | 56/100 [00:33<00:26,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  57%|█████▋    | 57/100 [00:34<00:25,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  58%|█████▊    | 58/100 [00:34<00:25,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  59%|█████▉    | 59/100 [00:35<00:24,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  60%|██████    | 60/100 [00:36<00:23,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  61%|██████    | 61/100 [00:36<00:23,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  62%|██████▏   | 62/100 [00:37<00:22,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  63%|██████▎   | 63/100 [00:37<00:22,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  64%|██████▍   | 64/100 [00:38<00:22,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  65%|██████▌   | 65/100 [00:39<00:21,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  66%|██████▌   | 66/100 [00:39<00:20,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  67%|██████▋   | 67/100 [00:40<00:19,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  68%|██████▊   | 68/100 [00:40<00:19,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  69%|██████▉   | 69/100 [00:41<00:18,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  70%|███████   | 70/100 [00:42<00:17,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  71%|███████   | 71/100 [00:42<00:17,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  72%|███████▏  | 72/100 [00:43<00:16,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  73%|███████▎  | 73/100 [00:43<00:16,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  74%|███████▍  | 74/100 [00:44<00:15,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  75%|███████▌  | 75/100 [00:45<00:15,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  76%|███████▌  | 76/100 [00:45<00:14,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  77%|███████▋  | 77/100 [00:46<00:13,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  78%|███████▊  | 78/100 [00:46<00:13,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  79%|███████▉  | 79/100 [00:47<00:12,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  80%|████████  | 80/100 [00:48<00:11,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  81%|████████  | 81/100 [00:48<00:11,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  82%|████████▏ | 82/100 [00:49<00:10,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  83%|████████▎ | 83/100 [00:49<00:10,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  84%|████████▍ | 84/100 [00:50<00:09,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  85%|████████▌ | 85/100 [00:51<00:09,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  86%|████████▌ | 86/100 [00:51<00:08,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  87%|████████▋ | 87/100 [00:52<00:07,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  88%|████████▊ | 88/100 [00:52<00:07,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  89%|████████▉ | 89/100 [00:53<00:06,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  90%|█████████ | 90/100 [00:54<00:06,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  91%|█████████ | 91/100 [00:54<00:05,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  92%|█████████▏| 92/100 [00:55<00:04,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  93%|█████████▎| 93/100 [00:56<00:04,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  94%|█████████▍| 94/100 [00:56<00:03,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  95%|█████████▌| 95/100 [00:57<00:02,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  96%|█████████▌| 96/100 [00:57<00:02,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  97%|█████████▋| 97/100 [00:58<00:01,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  98%|█████████▊| 98/100 [00:58<00:01,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  99%|█████████▉| 99/100 [00:59<00:00,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples: 100%|██████████| 100/100 [01:00<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 52.00%\n",
      "Example 1: contradiction\n",
      "Example 2: entailment\n",
      "Example 3: entailment\n",
      "Example 4: entailment\n",
      "Example 5: entailment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, max_length=70):\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    model.eval()\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "    # Progress bar with fixed position\n",
    "    for i in tqdm(range(len(dataset)), position=0, leave=True,desc=\"Predicting on test Samples\"):\n",
    "        premise = dataset['premise'][i]\n",
    "        hypothesis = dataset['hypothesis'][i]\n",
    "        label = dataset['label'][i]\n",
    "        \n",
    "        # Skip ambiguous label (-1) in SNLI dataset\n",
    "        if label == -1:\n",
    "            print(f\"Skipped example {i} (ambiguous label)\")\n",
    "            continue\n",
    "        true_labels.append(label_map[label])\n",
    "\n",
    "        # Concatenate premise and hypothesis with a more specific prompt\n",
    "        input_text = (\n",
    "            f\"Premise: {premise}\\n\"\n",
    "            f\"Hypothesis: {hypothesis}\\n\"\n",
    "            f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        # Tokenize input text\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=4)  # Strict limit on max_new_tokens to get concise answers\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            # Use regex to extract the label directly after \"Answer:\"\n",
    "            match = re.search(r\"Answer:\\s*(entailment|neutral|contradiction)\", prediction, re.IGNORECASE)\n",
    "            if match:\n",
    "                prediction = match.group(1).lower()\n",
    "            else:\n",
    "                prediction = \"neutral\"  # Default if no match is found\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy, predictions\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Evaluate model and calculate accuracy\n",
    "accuracy, predictions = evaluate_model(model, tokenizer, test_data)\n",
    "\n",
    "# Display accuracy and sample predictions\n",
    "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
    "for i, prediction in enumerate(predictions[:5]):\n",
    "    print(f\"Example {i + 1}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T15:10:12.139834Z",
     "iopub.status.busy": "2024-11-04T15:10:12.139529Z",
     "iopub.status.idle": "2024-11-04T15:10:12.147217Z",
     "shell.execute_reply": "2024-11-04T15:10:12.146199Z",
     "shell.execute_reply.started": "2024-11-04T15:10:12.139801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 1001\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T17:16:38.949455Z",
     "iopub.status.busy": "2024-11-04T17:16:38.949053Z",
     "iopub.status.idle": "2024-11-04T17:26:52.843979Z",
     "shell.execute_reply": "2024-11-04T17:26:52.839764Z",
     "shell.execute_reply.started": "2024-11-04T17:16:38.949419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0446d8ed88d4546851f65b5079b30f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c17d603eb4c4f66a664975646c4f887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584dfbfa4d0a4b2aa31719aa50df5710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  # expand paths, if not os.makedirs(\"~/bar\") will make directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 4362240 || Total parameters: 1525754880 || Percent trainable: 0.29%\n",
      "Calculating initial accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:31<00:00,  3.92s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/62 01:28 < 1:26:36, 0.01 it/s, Epoch 0.06/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 129\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Fine-tune and evaluate\u001b[39;00m\n\u001b[1;32m    122\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    123\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    124\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m )\n\u001b[0;32m--> 129\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Final evaluation after fine-tuning\u001b[39;00m\n\u001b[1;32m    132\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2390\u001b[0m     epoch_dataloader\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[1;32m   2392\u001b[0m \u001b[38;5;66;03m# Reset the past mems state at the beginning of each epoch if necessary.\u001b[39;00m\n\u001b[0;32m-> 2393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m steps_in_epoch \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2397\u001b[0m     \u001b[38;5;28mlen\u001b[39m(epoch_dataloader)\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m len_dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2400\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import psutil\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define tokenize function - returning dict to avoid list concatenation issue\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['premise'], batch['hypothesis'], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Lora configuration\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Track trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params} || Total parameters: {total_params} || Percent trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Custom trainer to handle unsupported 'label' argument\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"input_ids\").clone()  # Shift labels for causal LM\n",
    "        inputs[\"labels\"] = labels\n",
    "        inputs.pop(\"label\", None)  # Remove unsupported 'label' argument if exists\n",
    "        return super().compute_loss(model, inputs, return_outputs)\n",
    "\n",
    "# Training arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T17:27:21.574732Z",
     "iopub.status.busy": "2024-11-04T17:27:21.574298Z",
     "iopub.status.idle": "2024-11-04T18:13:20.614343Z",
     "shell.execute_reply": "2024-11-04T18:13:20.612392Z",
     "shell.execute_reply.started": "2024-11-04T17:27:21.574691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  # expand paths, if not os.makedirs(\"~/bar\") will make directory\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 44:13, Epoch 0.99/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 3511 has 14.71 GiB memory in use. Of the allocated memory 13.82 GiB is allocated by PyTorch, and 690.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 77\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Fine-tune and evaluate\u001b[39;00m\n\u001b[1;32m     70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     72\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Final evaluation after fine-tuning\u001b[39;00m\n\u001b[1;32m     80\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2487\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tr_loss\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m tr_loss_step\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   2485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated loss must be on the original device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but device in use is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss_step\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2487\u001b[0m         )\n\u001b[1;32m   2488\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss_step\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_flos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_point_ops(inputs))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2915\u001b[0m, in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2909\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m safetensors\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload_file(best_safe_model_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2911\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m   2912\u001b[0m         best_model_path,\n\u001b[1;32m   2913\u001b[0m         map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2914\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mweights_only_kwarg,\n\u001b[0;32m-> 2915\u001b[0m     )\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;66;03m# If the model is on the GPU, it still works!\u001b[39;00m\n\u001b[1;32m   2918\u001b[0m \u001b[38;5;66;03m# workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\u001b[39;00m\n\u001b[1;32m   2919\u001b[0m \u001b[38;5;66;03m# which takes *args instead of **kwargs\u001b[39;00m\n\u001b[1;32m   2920\u001b[0m load_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2872\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3868\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3866\u001b[0m     best_model_index \u001b[38;5;241m=\u001b[39m checkpoints_sorted\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mstr\u001b[39m(Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint)))\n\u001b[1;32m   3867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(best_model_index, \u001b[38;5;28mlen\u001b[39m(checkpoints_sorted) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m-> 3868\u001b[0m         checkpoints_sorted[i], checkpoints_sorted[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m checkpoints_sorted[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], checkpoints_sorted[i]\n\u001b[1;32m   3869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m checkpoints_sorted\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4061\u001b[0m, in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4279\u001b[0m, in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nested_gather\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 4279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4280\u001b[0m \u001b[38;5;124;03m    Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\u001b[39;00m\n\u001b[1;32m   4281\u001b[0m \u001b[38;5;124;03m    concatenating them to `gathered`\u001b[39;00m\n\u001b[1;32m   4282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 46\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m     45\u001b[0m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Remove unsupported 'label' argument if exists\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocast_smart_context_manager\u001b[39m(\u001b[38;5;28mself\u001b[39m, cache_enabled: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   3531\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m \u001b[38;5;124;03m    A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\u001b[39;00m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;124;03m    arguments, depending on the situation.\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cpu_amp:\n\u001b[1;32m   3536\u001b[0m         ctx_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcpu\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(cache_enabled\u001b[38;5;241m=\u001b[39mcache_enabled, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:187\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:204\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: Any, output_device: Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:109\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     gather_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:100\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll dicts must have the same number of keys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m<string>:8\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, loss, logits, past_key_values, hidden_states, attentions)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:390\u001b[0m, in \u001b[0;36mModelOutput.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# if we provided an iterator as first field and the iterator is a (key, value) iterator\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# set the associated fields\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_field_iterator:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterator):\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(element) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    394\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    395\u001b[0m         ):\n\u001b[1;32m    396\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    397\u001b[0m                 \u001b[38;5;66;03m# If we do not have an iterator of key/values, set it as attribute\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:100\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll dicts must have the same number of keys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)((k, \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    101\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m out)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:104\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:104\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)\u001b[38;5;241m.\u001b[39m_make(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:94\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGather\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:75\u001b[0m, in \u001b[0;36mGather.forward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ctx\u001b[38;5;241m.\u001b[39munsqueezed_scalar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i\u001b[38;5;241m.\u001b[39msize(ctx\u001b[38;5;241m.\u001b[39mdim) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py:235\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination, out)\u001b[0m\n\u001b[1;32m    228\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing -1 to represent CPU tensor is deprecated. Please use a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    230\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice object or string instead, e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    232\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    233\u001b[0m         )\n\u001b[1;32m    234\u001b[0m     destination \u001b[38;5;241m=\u001b[39m _get_device_index(destination, allow_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m destination \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 3511 has 14.71 GiB memory in use. Of the allocated memory 13.82 GiB is allocated by PyTorch, and 690.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "output_dir = \"./snli_finetune_phi2\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    # batch_size=16\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    # wandb=False\"\n",
    "    \n",
    "    learning_rate=2.5e-5,\n",
    "    logging_steps=50,\n",
    "    save_steps=len(train_dataset) // 2,\n",
    "    eval_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    #bits=4\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Mapping for SNLI label IDs to text labels\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\", -1: \"neutral\"}\n",
    "\n",
    "# Function to calculate accuracy and log failure cases\n",
    "def evaluate_model(model, dataset, tokenizer, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    results = []\n",
    "\n",
    "    for idx, sample in tqdm(enumerate(dataset), total=total):\n",
    "        input_text = (\n",
    "            f\"Premise: {sample['premise']}\\n\"\n",
    "            f\"Hypothesis: {sample['hypothesis']}\\n\"\n",
    "            f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
    "        )\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "        target_label = label_map[sample['label']]  # Convert label ID to text label\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=50,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        match = re.search(r\"Answer:\\s*(entailment|neutral|contradiction)\", output_text, re.IGNORECASE)\n",
    "        prediction = match.group(1).lower() if match else \"neutral\"  # Default if no match found\n",
    "\n",
    "        # Store result details for later analysis\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"prediction\": prediction,\n",
    "            \"target_label\": target_label,\n",
    "            \"is_correct\": prediction == target_label\n",
    "        })\n",
    "        if prediction == target_label:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, results\n",
    "\n",
    "# Record initial accuracy and resources\n",
    "start_time = time.time()\n",
    "cpu_memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else None\n",
    "\n",
    "\n",
    "# Fine-tune and evaluate\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Final evaluation after fine-tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T18:15:00.175950Z",
     "iopub.status.busy": "2024-11-04T18:15:00.175539Z",
     "iopub.status.idle": "2024-11-04T18:21:39.334138Z",
     "shell.execute_reply": "2024-11-04T18:21:39.333100Z",
     "shell.execute_reply.started": "2024-11-04T18:15:00.175911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:12<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 47.64 minutes\n",
      "CPU Memory Used: 11.73 GB\n",
      "GPU Memory Used: 5.20 GB\n",
      "Initial Model Accuracy: 52.00%\n",
      "Fine-tuned Model Accuracy: 54.00%\n",
      "\n",
      "Example failure cases before fine-tuning:\n",
      "Sample 1:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church has cracks in the ceiling.\n",
      "Generated Label (Pre-trained): Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church has cracks in the ceiling.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: Contradiction\n",
      "\n",
      "Exercise 2:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church choir is made up of only men.\n",
      "Answer\n",
      "Sample 2:\n",
      "Premise: A woman within an orchestra is playing a violin.\n",
      "Hypothesis: A woman is playing the violin.\n",
      "Generated Label (Pre-trained): Premise: A woman within an orchestra is playing a violin.\n",
      "Hypothesis: A woman is playing the violin.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: entailment\n",
      "\n",
      "Exercise 2:\n",
      "Premise: A man is playing the trumpet.\n",
      "Hypothesis: A man is playing the trumpet.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: entail\n",
      "Sample 3:\n",
      "Premise: Two men climbing on a wooden scaffold.\n",
      "Hypothesis: Two sad men climbing on a wooden scaffold.\n",
      "Generated Label (Pre-trained): Premise: Two men climbing on a wooden scaffold.\n",
      "Hypothesis: Two sad men climbing on a wooden scaffold.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: entailment\n",
      "\n",
      "Exercise 2:\n",
      "Premise: A man is holding a gun.\n",
      "Hypothesis: The man is about to shoot someone.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer:\n",
      "\n",
      "Example failure cases corrected by fine-tuned model:\n",
      "Sample 1:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church has cracks in the ceiling.\n",
      "Generated Label (Fine-tuned): Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church has cracks in the ceiling.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: Contradiction\n",
      "\n",
      "Exercise 2:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church choir is made up of only men.\n",
      "Answer\n",
      "Sample 2:\n",
      "Premise: A woman within an orchestra is playing a violin.\n",
      "Hypothesis: A woman is playing the violin.\n",
      "Generated Label (Fine-tuned): Premise: A woman within an orchestra is playing a violin.\n",
      "Hypothesis: A woman is playing the violin.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: entailment\n",
      "\n",
      "Exercise 2:\n",
      "Premise: A man is playing the trumpet.\n",
      "Hypothesis: A man is playing the trumpet.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: entail\n",
      "Sample 3:\n",
      "Premise: Two men climbing on a wooden scaffold.\n",
      "Hypothesis: Two sad men climbing on a wooden scaffold.\n",
      "Generated Label (Fine-tuned): Premise: Two men climbing on a wooden scaffold.\n",
      "Hypothesis: Two sad men climbing on a wooden scaffold.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer: entailment\n",
      "\n",
      "Exercise 2:\n",
      "Premise: A man is holding a gun.\n",
      "Hypothesis: The man is about to shoot someone.\n",
      "Answer with one of the following: entailment, neutral, contradiction.\n",
      "Answer:\n",
      "\n",
      "\n",
      "Indexes of cases still incorrect after fine-tuning: [0, 2, 3, 4, 6, 9, 14, 15, 16, 20, 21, 22, 25, 32, 33, 35, 36, 41, 42, 44, 46, 47, 48, 52, 54, 55, 57, 58, 59, 61, 63, 65, 66, 68, 73, 74, 78, 81, 84, 85, 88, 89, 92, 95, 97, 99]\n",
      "Indexes of cases correct initially and stayed correct: [1, 5, 7, 8, 10, 11, 12, 13, 17, 18, 19, 23, 24, 26, 27, 28, 29, 30, 31, 34, 37, 38, 39, 40, 43, 45, 49, 50, 51, 53, 56, 60, 62, 64, 67, 69, 70, 71, 72, 75, 76, 77, 79, 80, 82, 83, 86, 87, 90, 91, 93, 94, 96, 98]\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "fine_tuned_accuracy, fine_tuned_results = evaluate_model(model, test_dataset, tokenizer, device)\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "# Identify cases with improvement, still incorrect, and initially correct\n",
    "improved_cases = []\n",
    "still_incorrect_cases = []\n",
    "initially_correct_cases = []\n",
    "\n",
    "for init, final in zip(initial_results, fine_tuned_results):\n",
    "    if not init[\"is_correct\"] and final[\"is_correct\"]:\n",
    "        improved_cases.append(init[\"index\"])\n",
    "    elif not init[\"is_correct\"] and not final[\"is_correct\"]:\n",
    "        still_incorrect_cases.append(init[\"index\"])\n",
    "    elif init[\"is_correct\"] and final[\"is_correct\"]:\n",
    "        initially_correct_cases.append(init[\"index\"])\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "\n",
    "# Print resources and final results\n",
    "print(f\"Training completed in {training_duration/60:.2f} minutes\")\n",
    "print(f\"CPU Memory Used: {cpu_memory:.2f} GB\")\n",
    "if gpu_memory:\n",
    "    print(f\"GPU Memory Used: {gpu_memory:.2f} GB\")\n",
    "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
    "print(f\"Fine-tuned Model Accuracy: {fine_tuned_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print example failure cases and category indexes\n",
    "print(\"\\nExample failure cases before fine-tuning:\")\n",
    "for i in range(3):\n",
    "    sample = test_dataset[i]\n",
    "    input_text = (\n",
    "        f\"Premise: {sample['premise']}\\n\"\n",
    "        f\"Hypothesis: {sample['hypothesis']}\\n\"\n",
    "        f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Premise: {sample['premise']}\")\n",
    "    print(f\"Hypothesis: {sample['hypothesis']}\")\n",
    "    print(f\"Generated Label (Pre-trained): {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "print(\"\\nExample failure cases corrected by fine-tuned model:\")\n",
    "for i in range(3):\n",
    "    sample = test_dataset[i]\n",
    "    input_text = (\n",
    "        f\"Premise: {sample['premise']}\\n\"\n",
    "        f\"Hypothesis: {sample['hypothesis']}\\n\"\n",
    "        f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Premise: {sample['premise']}\")\n",
    "    print(f\"Hypothesis: {sample['hypothesis']}\")\n",
    "    print(f\"Generated Label (Fine-tuned): {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# Print lists of case indexes\n",
    "\n",
    "print(f\"Indexes of cases still incorrect after fine-tuning: {still_incorrect_cases}\")\n",
    "print(f\"Indexes of cases correct initially and stayed correct: {initially_correct_cases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T18:22:58.004870Z",
     "iopub.status.busy": "2024-11-04T18:22:58.004324Z",
     "iopub.status.idle": "2024-11-04T18:23:50.038628Z",
     "shell.execute_reply": "2024-11-04T18:23:50.037779Z",
     "shell.execute_reply.started": "2024-11-04T18:22:58.004819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test Samples:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   1%|          | 1/100 [00:00<00:56,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   2%|▏         | 2/100 [00:01<00:52,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   3%|▎         | 3/100 [00:01<00:50,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   4%|▍         | 4/100 [00:02<00:51,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   5%|▌         | 5/100 [00:02<00:49,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   6%|▌         | 6/100 [00:03<00:48,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   7%|▋         | 7/100 [00:03<00:47,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   8%|▊         | 8/100 [00:04<00:47,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:   9%|▉         | 9/100 [00:04<00:46,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  10%|█         | 10/100 [00:05<00:46,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  11%|█         | 11/100 [00:05<00:45,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  12%|█▏        | 12/100 [00:06<00:45,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  13%|█▎        | 13/100 [00:06<00:46,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  14%|█▍        | 14/100 [00:07<00:45,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  15%|█▌        | 15/100 [00:07<00:44,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  16%|█▌        | 16/100 [00:08<00:43,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  17%|█▋        | 17/100 [00:08<00:43,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  18%|█▊        | 18/100 [00:09<00:42,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  19%|█▉        | 19/100 [00:09<00:41,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  20%|██        | 20/100 [00:10<00:41,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  21%|██        | 21/100 [00:10<00:40,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  22%|██▏       | 22/100 [00:11<00:39,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  23%|██▎       | 23/100 [00:11<00:39,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  24%|██▍       | 24/100 [00:12<00:38,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  25%|██▌       | 25/100 [00:12<00:37,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  26%|██▌       | 26/100 [00:13<00:39,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  27%|██▋       | 27/100 [00:14<00:38,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  28%|██▊       | 28/100 [00:14<00:37,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  29%|██▉       | 29/100 [00:15<00:36,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  30%|███       | 30/100 [00:15<00:36,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  31%|███       | 31/100 [00:16<00:35,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  32%|███▏      | 32/100 [00:16<00:36,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  33%|███▎      | 33/100 [00:17<00:35,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  34%|███▍      | 34/100 [00:17<00:34,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  35%|███▌      | 35/100 [00:18<00:33,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  36%|███▌      | 36/100 [00:18<00:32,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  37%|███▋      | 37/100 [00:19<00:32,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  38%|███▊      | 38/100 [00:19<00:31,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  39%|███▉      | 39/100 [00:20<00:32,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  40%|████      | 40/100 [00:20<00:33,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  41%|████      | 41/100 [00:21<00:31,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  42%|████▏     | 42/100 [00:21<00:30,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  43%|████▎     | 43/100 [00:22<00:29,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  44%|████▍     | 44/100 [00:22<00:29,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  45%|████▌     | 45/100 [00:23<00:28,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  46%|████▌     | 46/100 [00:23<00:27,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  47%|████▋     | 47/100 [00:24<00:27,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  48%|████▊     | 48/100 [00:24<00:26,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  49%|████▉     | 49/100 [00:25<00:26,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  50%|█████     | 50/100 [00:25<00:25,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  51%|█████     | 51/100 [00:26<00:25,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  52%|█████▏    | 52/100 [00:27<00:24,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  53%|█████▎    | 53/100 [00:27<00:24,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  54%|█████▍    | 54/100 [00:28<00:23,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  55%|█████▌    | 55/100 [00:28<00:23,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  56%|█████▌    | 56/100 [00:29<00:22,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  57%|█████▋    | 57/100 [00:29<00:22,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  58%|█████▊    | 58/100 [00:30<00:21,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  59%|█████▉    | 59/100 [00:30<00:21,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  60%|██████    | 60/100 [00:31<00:20,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  61%|██████    | 61/100 [00:31<00:20,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  62%|██████▏   | 62/100 [00:32<00:19,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  63%|██████▎   | 63/100 [00:32<00:20,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  64%|██████▍   | 64/100 [00:33<00:19,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  65%|██████▌   | 65/100 [00:33<00:18,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  66%|██████▌   | 66/100 [00:34<00:17,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  67%|██████▋   | 67/100 [00:34<00:17,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  68%|██████▊   | 68/100 [00:35<00:16,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  69%|██████▉   | 69/100 [00:35<00:16,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  70%|███████   | 70/100 [00:36<00:15,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  71%|███████   | 71/100 [00:36<00:14,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  72%|███████▏  | 72/100 [00:37<00:14,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  73%|███████▎  | 73/100 [00:37<00:13,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  74%|███████▍  | 74/100 [00:38<00:13,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  75%|███████▌  | 75/100 [00:39<00:13,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  76%|███████▌  | 76/100 [00:39<00:12,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  77%|███████▋  | 77/100 [00:40<00:12,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  78%|███████▊  | 78/100 [00:40<00:11,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  79%|███████▉  | 79/100 [00:41<00:10,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  80%|████████  | 80/100 [00:41<00:10,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  81%|████████  | 81/100 [00:42<00:09,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  82%|████████▏ | 82/100 [00:42<00:09,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  83%|████████▎ | 83/100 [00:43<00:08,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  84%|████████▍ | 84/100 [00:43<00:08,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  85%|████████▌ | 85/100 [00:44<00:07,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  86%|████████▌ | 86/100 [00:44<00:07,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  87%|████████▋ | 87/100 [00:45<00:06,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  88%|████████▊ | 88/100 [00:45<00:06,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  89%|████████▉ | 89/100 [00:46<00:05,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  90%|█████████ | 90/100 [00:46<00:05,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  91%|█████████ | 91/100 [00:47<00:04,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  92%|█████████▏| 92/100 [00:47<00:04,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  93%|█████████▎| 93/100 [00:48<00:03,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  94%|█████████▍| 94/100 [00:48<00:03,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  95%|█████████▌| 95/100 [00:49<00:02,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  96%|█████████▌| 96/100 [00:49<00:02,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  97%|█████████▋| 97/100 [00:50<00:01,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  98%|█████████▊| 98/100 [00:50<00:01,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples:  99%|█████████▉| 99/100 [00:51<00:00,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Predicting on test Samples: 100%|██████████| 100/100 [00:52<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 54.00%\n",
      "Example 1: contradiction\n",
      "Example 2: entailment\n",
      "Example 3: entailment\n",
      "Example 4: entailment\n",
      "Example 5: entailment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, max_length=70):\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    model.eval()\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "    # Progress bar with fixed position\n",
    "    for i in tqdm(range(len(dataset)), position=0, leave=True,desc=\"Predicting on test Samples\"):\n",
    "        premise = dataset['premise'][i]\n",
    "        hypothesis = dataset['hypothesis'][i]\n",
    "        label = dataset['label'][i]\n",
    "        \n",
    "        # Skip ambiguous label (-1) in SNLI dataset\n",
    "        if label == -1:\n",
    "            print(f\"Skipped example {i} (ambiguous label)\")\n",
    "            continue\n",
    "        true_labels.append(label_map[label])\n",
    "\n",
    "        # Concatenate premise and hypothesis with a more specific prompt\n",
    "        input_text = (\n",
    "            f\"Premise: {premise}\\n\"\n",
    "            f\"Hypothesis: {hypothesis}\\n\"\n",
    "            f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        # Tokenize input text\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=4)  # Strict limit on max_new_tokens to get concise answers\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            # Use regex to extract the label directly after \"Answer:\"\n",
    "            match = re.search(r\"Answer:\\s*(entailment|neutral|contradiction)\", prediction, re.IGNORECASE)\n",
    "            if match:\n",
    "                prediction = match.group(1).lower()\n",
    "            else:\n",
    "                prediction = \"neutral\"  # Default if no match is found\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy, predictions\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Evaluate model and calculate accuracy\n",
    "accuracy, predictions = evaluate_model(model, tokenizer, test_data)\n",
    "\n",
    "# Display accuracy and sample predictions\n",
    "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
    "for i, prediction in enumerate(predictions[:5]):\n",
    "    print(f\"Example {i + 1}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T15:10:12.149021Z",
     "iopub.status.busy": "2024-11-04T15:10:12.148445Z",
     "iopub.status.idle": "2024-11-04T15:10:12.162045Z",
     "shell.execute_reply": "2024-11-04T15:10:12.161120Z",
     "shell.execute_reply.started": "2024-11-04T15:10:12.148974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2621440 || all params: 1524014080 || trainable%: 0.1720089095239855\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "print(print_trainable_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T15:10:12.165609Z",
     "iopub.status.busy": "2024-11-04T15:10:12.165032Z",
     "iopub.status.idle": "2024-11-04T15:10:12.175837Z",
     "shell.execute_reply": "2024-11-04T15:10:12.174887Z",
     "shell.execute_reply.started": "2024-11-04T15:10:12.165575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 2621440\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "total_params = count_parameters(pretrained_model)\n",
    "print(f\"Total parameters in the model: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T16:30:44.623025Z",
     "iopub.status.busy": "2024-11-04T16:30:44.622405Z",
     "iopub.status.idle": "2024-11-04T16:30:44.632713Z",
     "shell.execute_reply": "2024-11-04T16:30:44.631617Z",
     "shell.execute_reply.started": "2024-11-04T16:30:44.622982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This church choir sings to the masses as they sing joyous songs from the book at a church.',\n",
       " 'A woman within an orchestra is playing a violin.',\n",
       " 'Two men climbing on a wooden scaffold.',\n",
       " 'A man in a black shirt, in a commercial kitchen, holding up meat he took out of a bag.',\n",
       " 'a woman in a black shirt looking at a bicycle.',\n",
       " 'many children play in the water.',\n",
       " 'A group of people stand near and on a large black square on the ground with some yellow writing on it.',\n",
       " 'A female softball player wearing blue and red crouches in the infield, waiting for the next play.',\n",
       " 'Workers standing on a lift.',\n",
       " 'Two men in neon yellow shirts busily sawing a log in half.',\n",
       " 'A Skier ski-jumping while two other skiers watch his act.',\n",
       " 'Children bathe in water from large drums.',\n",
       " 'A woman is standing near three stores, two have beautiful artwork and the other store has Largo written on it.',\n",
       " 'People are all standing together in front of a statue of an animal, and they are all wearing cool-weather clothing.',\n",
       " 'A man is renovating a room.',\n",
       " 'Military personnel are shopping',\n",
       " 'An Ambulance is passing a man wearing a bandanna and girl.',\n",
       " 'Three soccer players, two in orange one in yellow, running for the ball on a soccer field.',\n",
       " 'The Sooner football player carrying the ball is trying to avoid being tackled.',\n",
       " 'An older gentleman wearing a hat is walking on crutches next to a busy street.',\n",
       " 'A small child is riding in a red wagon.',\n",
       " 'A female marathon runner wearing a red headband, a red tank top and black shorts jogging down a paved road.',\n",
       " 'Two men playing in a beautiful lake surrounded by mountains.',\n",
       " 'Man in chair laughing and talking to others, while handling books.',\n",
       " 'Two middle-aged police officers watch over a parking lot, at night.',\n",
       " 'A bicycle rider wearing racing gear pedals a yellow bike past the wire fence at the edge of a field, with a stand of trees in the background.',\n",
       " 'A person with a green backpack is riding a bike down the road.',\n",
       " 'A man with a tan jacket with a full grocery bag is crossing the street.',\n",
       " 'A man in a purple mascot costume is standing outside of a store while a man and a woman each wearing flamboyant clothing stand off to the side.',\n",
       " 'A team of surgeons operate on a female patient.',\n",
       " 'Bubbles surround a statue in the middle of a street.',\n",
       " 'A man in a tie-dyed shirt and jeans is sitting on a bench with a dog and a guitar on his lap, as well as a harmonica near his mouth.',\n",
       " 'A young man with glasses and his companion are attempting to do laundry in a public place.',\n",
       " 'A person obscured in shadow in a gymnasium.',\n",
       " 'Two people sitting on the sand.',\n",
       " 'A man in an orange jacket reaches under a busted up blue car on wooden supports.',\n",
       " 'A girl with a bowing throwing a wet sponge.',\n",
       " 'Two boys inside a fence jump in the air while holding a basketball.',\n",
       " 'A woman in an American military uniform sits at a table and writes the words \"sad,\" \"depressed,\" and \"hatred\" on a large sheet of white paper.',\n",
       " 'A young redheaded girl, wearing a yellow shirt, black pants, and sneakers, jumping in a grassy field with blue skies and wispy clouds in the background.',\n",
       " 'Man with black shirt and sunglasses makes something out of a balloon.',\n",
       " 'I am squatting on a dock, looking into a lake.',\n",
       " 'Two women wearing aprons and hairnets look at each other while they reach into metal canisters.',\n",
       " 'A man is sitting on a blue bench with a blue blanket covering his face.',\n",
       " 'A red dog jumps and catches a tennis ball in its mouth.',\n",
       " 'A man wearing red ski pants, a black jacket, and a white helmet is skiing down a mountain.',\n",
       " 'Several people wearing green shirts walk on the beach together.',\n",
       " 'A group of African Americans getting ready to eat.',\n",
       " 'Two hikers are backpacking up a snow slope.',\n",
       " 'A female swimmer wearing a swimming cap does the butterfly stroke.',\n",
       " 'A woman in a black hooded sweatshirt walking with a large dog.',\n",
       " 'A child hugs a birdhouse.',\n",
       " 'A gray-haired man rides a bike in front of a bus advertising Ironman.',\n",
       " 'Group of young adults posing for picture near spanish-language sign.',\n",
       " 'A man sitting and a woman laying in his lap kissing each other.',\n",
       " 'man doing carpentry or construction on top of an unfinished building.',\n",
       " 'A woman sweeping in front of a ladder on a busy street.',\n",
       " 'A light technician man with tribal tattoos aiming a spotlight over a balcony.',\n",
       " 'It looks like quite a sweaty, smelly dog pile over one little rugby ball, but the boys in blue seem to want it more.',\n",
       " 'A woman is holding a microphone in one hand and her mouth is open.',\n",
       " 'A five piece horn band all playing in a hall of what looks like a church.',\n",
       " 'A group of men and women are sitting at a table eating and drinking.',\n",
       " 'A woman in a red shirt looks at a map while with a view of a river and several boats in the background.',\n",
       " 'A woman in a dress is singing and having a good time.',\n",
       " 'A group of men playing rugby on the sand.',\n",
       " 'A man siting on a bench with a briefcase.',\n",
       " 'A man with two small boys making a purchase from a woman.',\n",
       " 'A group of children playing with props',\n",
       " 'A man and a woman are holding hands on the shore of a lake.',\n",
       " 'A man in a white t-shirt and jeans is holding a mallet and chisel next to his abstract sculpture which stands on several bricks.',\n",
       " 'A man and a child are laughing at each other.',\n",
       " 'A man in a plaid red shirt casts his fishing line out into the water.',\n",
       " 'City street crowded with sports fans wearing orange.',\n",
       " 'A tattooed woman clicking on a mouse on a desk.',\n",
       " 'A street performer is entertaining a gathered audience with the help of a young boy.',\n",
       " 'A guy in a red jacket is snowboarding in midair.',\n",
       " 'A man in a lab coat is looking through a microscope.',\n",
       " 'Passengers in a rusty yellow car driving down the street.',\n",
       " 'Two women are hugging on a path through a grassy area with a cow visible past them.',\n",
       " \"It's another day of celebration, with a parade and signs, with many people in attendance.\",\n",
       " 'A senior citizen wearing a hat, blue button up shirt, Khaki shorts and sandals walking in a park holding two ice cream cones.',\n",
       " 'A man with a gray beard and a little boy are sitting on the floor looking over some papers in a room with a bunk bed.',\n",
       " 'The man in the blue shirt is relaxing on the rocks.',\n",
       " 'A man wearing a purple cap, yellow snow goggles, a periwinkle jacket and red backpack moves quickly through powdery snow near a winter tree.',\n",
       " 'Men wearing hats walk on the street.',\n",
       " 'The dog is in the snow in front of some trees.',\n",
       " 'A man falling off a bull as the animal jumps into the air.',\n",
       " 'Overly dramatic couple pose for a picture where an \"angry \"man \"chokes\" a woman who sticks out her tongue.',\n",
       " 'Two women are eating lollipops- the blond woman is wearing a button and the one with brown hair and a scarf has her eyes closed.',\n",
       " 'Three performers are on the stage floor in black lace costumes.',\n",
       " 'A woman in a blue jacket dragging a child on a sled through the snow.',\n",
       " 'A group of students are walking through the campus.',\n",
       " 'A black woman is sitting on the beach examining an octopus.',\n",
       " 'A man on a sidewalk is playing the accordion while happy people pass by.',\n",
       " 'Large brown dog walking in shallow water.',\n",
       " 'A shirt booth with a man printing a shirt.',\n",
       " 'A child stuck up in a tree.',\n",
       " 'Several women in headscarves are standing in a cobbled courtyard.',\n",
       " 'A person attempts to rope a black cow while riding a horse.',\n",
       " 'An impoverished person, wearing a torn brown shirt and no shoes, is kneeling down along the banks of a trash infested body of water.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"premise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
